---
title: "Soft Threshold Weight Reparameterization for Learnable Sparsity"
collection: publications
permalink: /publications/2020-05-31-STR-5
excerpt: 'The paper has been accepted for a presentation.'
date: 2020-05-31
venue: 'International Conference on Machine Learning (ICML), 2020'
year: '2020'
authors: 'Aditya Kusupati, Vivek Ramanujan*, Raghav Somani*, Mitchell Wortsman*, Prateek Jain, Sham Kakade & Ali Farhadi'
arxiv: 'https://arxiv.org/abs/2002.03231'
bib: 'https://ui.adsabs.harvard.edu/abs/2020arXiv200203231K'
code: 'https://github.com/RAIVNLab/STR'

---
Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics.

The paper has been accepted at the [ICML 2020](https://icml.cc/Conferences/2020){:target="_blank"}.

Please find the below resources
1. [ArXiv](https://arxiv.org/abs/2002.03231){:target="_blank"}.
2. [Code and pretrained models](https://github.com/RAIVNLab/STR){:target="_blank"}.